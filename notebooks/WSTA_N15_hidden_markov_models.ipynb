{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook for COMP90042, Web search and Text Analysis*\n",
    "\n",
    "*Copyright The University of Melbourne, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll show how the Viterbi algorithm works for HMMs, assuming we have a trained model to start with. We will use the example in the JM3 book (Ch. 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model parameters based on the example from the slides/book (values taken from figure). Notice that here we explicitly split the initial probabilities \"pi\" from the transition matrix \"A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = NNP, MD, VB, JJ, NN, RB, DT = 0, 1, 2, 3, 4, 5, 6\n",
    "tag_dict = {0: 'NNP',\n",
    "           1: 'MD',\n",
    "           2: 'VB',\n",
    "           3: 'JJ',\n",
    "           4: 'NN',\n",
    "           5: 'RB',\n",
    "           6: 'DT'}\n",
    "words = Janet, will, back, the, bill = 0, 1, 2, 3, 4\n",
    "\n",
    "A = np.array([\n",
    "    [0.3777, 0.0110, 0.0009, 0.0084, 0.0584, 0.0090, 0.0025],\n",
    "    [0.0008, 0.0002, 0.7968, 0.0005, 0.0008, 0.1698, 0.0041],\n",
    "    [0.0322, 0.0005, 0.0050, 0.0837, 0.0615, 0.0514, 0.2231],\n",
    "    [0.0366, 0.0004, 0.0001, 0.0733, 0.4509, 0.0036, 0.0036],\n",
    "    [0.0096, 0.0176, 0.0014, 0.0086, 0.1216, 0.0177, 0.0068],\n",
    "    [0.0068, 0.0102, 0.1011, 0.1012, 0.0120, 0.0728, 0.0479],\n",
    "    [0.1147, 0.0021, 0.0002, 0.2157, 0.4744, 0.0102, 0.0017]\n",
    "    ])\n",
    "\n",
    "pi = np.array([0.2767, 0.0006, 0.0031, 0.0453, 0.0449, 0.0510, 0.2026])\n",
    "\n",
    "B = np.array([\n",
    "    [0.000032, 0, 0, 0.000048, 0],\n",
    "    [0, 0.308431, 0, 0, 0],\n",
    "    [0, 0.000028, 0.000672, 0, 0.000028],\n",
    "    [0, 0, 0.000340, 0.000097, 0],\n",
    "    [0, 0.000200, 0.000223, 0.000006, 0.002337],\n",
    "    [0, 0, 0.010446, 0, 0],\n",
    "    [0, 0, 0, 0.506099, 0]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll code the Viterbi algorithm. It keeps a store of two components, the best scores to reach a state at a give time, and the last step of the path to get there. Scores alpha are initialised to -inf to denote that we haven't set them yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.zeros((len(tags), len(words))) # states x time steps\n",
    "alpha[:,:] = float('-inf')\n",
    "backpointers = np.zeros((len(tags), len(words)), 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base case for the recursion sets the starting state probs based on pi and generating the observation. (Note: we also change Numpy precision when printing for better viewing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.85e-06     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]\n",
      " [0.00e+00     -inf     -inf     -inf     -inf]]\n"
     ]
    }
   ],
   "source": [
    "# base case, time step 0\n",
    "alpha[:, 0] = pi * B[:,Janet]\n",
    "np.set_printoptions(precision=2)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the recursive step, where we maximise over incoming transitions reusing the best incoming score, computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.85e-06 0.00e+00     -inf     -inf     -inf]\n",
      " [0.00e+00 3.00e-08     -inf     -inf     -inf]\n",
      " [0.00e+00 2.23e-13     -inf     -inf     -inf]\n",
      " [0.00e+00 0.00e+00     -inf     -inf     -inf]\n",
      " [0.00e+00 1.03e-10     -inf     -inf     -inf]\n",
      " [0.00e+00 0.00e+00     -inf     -inf     -inf]\n",
      " [0.00e+00 0.00e+00     -inf     -inf     -inf]]\n"
     ]
    }
   ],
   "source": [
    "# time step 1\n",
    "for t1 in tags:\n",
    "    for t0 in tags:\n",
    "        score = alpha[t0, 0] * A[t0, t1] * B[t1, will]\n",
    "        if score > alpha[t1, 1]:\n",
    "            alpha[t1, 1] = score\n",
    "            backpointers[t1, 1] = t0\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the running maximum for any incoming state (t0) is maintained in alpha[1,t1], and the winning state is stored in addition, as a backpointer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with the next observations. (We'd do this as a loop over positions in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.85e-06 0.00e+00 0.00e+00     -inf     -inf]\n",
      " [0.00e+00 3.00e-08 0.00e+00     -inf     -inf]\n",
      " [0.00e+00 2.23e-13 1.61e-11     -inf     -inf]\n",
      " [0.00e+00 0.00e+00 5.11e-15     -inf     -inf]\n",
      " [0.00e+00 1.03e-10 5.36e-15     -inf     -inf]\n",
      " [0.00e+00 0.00e+00 5.33e-11     -inf     -inf]\n",
      " [0.00e+00 0.00e+00 0.00e+00     -inf     -inf]]\n",
      "[[8.85e-06 0.00e+00 0.00e+00 2.49e-17     -inf]\n",
      " [0.00e+00 3.00e-08 0.00e+00 0.00e+00     -inf]\n",
      " [0.00e+00 2.23e-13 1.61e-11 0.00e+00     -inf]\n",
      " [0.00e+00 0.00e+00 5.11e-15 5.23e-16     -inf]\n",
      " [0.00e+00 1.03e-10 5.36e-15 5.94e-18     -inf]\n",
      " [0.00e+00 0.00e+00 5.33e-11 0.00e+00     -inf]\n",
      " [0.00e+00 0.00e+00 0.00e+00 1.82e-12     -inf]]\n",
      "[[8.85e-06 0.00e+00 0.00e+00 2.49e-17 0.00e+00]\n",
      " [0.00e+00 3.00e-08 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 2.23e-13 1.61e-11 0.00e+00 1.02e-20]\n",
      " [0.00e+00 0.00e+00 5.11e-15 5.23e-16 0.00e+00]\n",
      " [0.00e+00 1.03e-10 5.36e-15 5.94e-18 2.01e-15]\n",
      " [0.00e+00 0.00e+00 5.33e-11 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 1.82e-12 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "# time step 2\n",
    "for t2 in tags:\n",
    "    for t1 in tags:\n",
    "        score = alpha[t1, 1] * A[t1, t2] * B[t2, back]\n",
    "        if score > alpha[t2, 2]:\n",
    "            alpha[t2, 2] = score\n",
    "            backpointers[t2, 2] = t1\n",
    "print(alpha)\n",
    "\n",
    "# time step 3\n",
    "for t3 in tags:\n",
    "    for t2 in tags:\n",
    "        score = alpha[t2, 2] * A[t2, t3] * B[t3, the]\n",
    "        if score > alpha[t3, 3]:\n",
    "            alpha[t3, 3] = score\n",
    "            backpointers[t3, 3] = t2\n",
    "print(alpha)\n",
    "\n",
    "# time step 4\n",
    "for t4 in tags:\n",
    "    for t3 in tags:\n",
    "        score = alpha[t3, 3] * A[t3, t4] * B[t4, bill]\n",
    "        if score > alpha[t4, 4]:\n",
    "            alpha[t4, 4] = score\n",
    "            backpointers[t4, 4] = t3\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read of the best final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n"
     ]
    }
   ],
   "source": [
    "t4 = np.argmax(alpha[:, 4])\n",
    "print(tag_dict[t4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out the rest of the path which is the best way to reach the final state, t2. We can work this out by taking a step backwards looking at the best incoming edge, i.e., as stored in the backpointers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n"
     ]
    }
   ],
   "source": [
    "t3 = backpointers[t4, 4]\n",
    "print(tag_dict[t3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this until we reach the start of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB\n",
      "MD\n",
      "NNP\n"
     ]
    }
   ],
   "source": [
    "t2 = backpointers[t3, 3]\n",
    "print(tag_dict[t2])\n",
    "t1 = backpointers[t2, 2]\n",
    "print(tag_dict[t1])\n",
    "t0 = backpointers[t1, 1]\n",
    "print(tag_dict[t0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. The best state sequence is t = [NNP MD VB DT NN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalising things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this all into a function to handle arbitrary length inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(params, words):\n",
    "    pi, A, B = params\n",
    "    N = len(words)\n",
    "    T = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((T, N))\n",
    "    alpha[:, :] = float('-inf')\n",
    "    backpointers = np.zeros((T, N), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[:, 0] = pi * B[:, words[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for w in range(1, N):\n",
    "        for t2 in range(T):\n",
    "            for t1 in range(T):\n",
    "                score = alpha[t1, w-1] * A[t1, t2] * B[t2, words[w]]\n",
    "                if score > alpha[t2, w]:\n",
    "                    alpha[t2, w] = score\n",
    "                    backpointers[t2, w] = t1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    output = []\n",
    "    output.append(np.argmax(alpha[:, N-1]))\n",
    "    for i in range(N-1, 0, -1):\n",
    "        output.append(backpointers[output[-1], i])\n",
    "    \n",
    "    return list(reversed(output)), np.max(alpha[:, N-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method on the same input, and a longer input observation sequence. Notice that we are using only 5 words as the vocabulary so we have to restrict tests to sentences containing only these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'MD', 'VB', 'DT', 'NN']\n",
      "2.013570710221386e-15\n"
     ]
    }
   ],
   "source": [
    "output, score = viterbi((pi, A, B), [Janet, will, back, the, bill])\n",
    "print([tag_dict[o] for o in output])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'MD', 'VB', 'DT', 'NNP', 'NN', 'NN']\n",
      "2.4671007551487516e-26\n"
     ]
    }
   ],
   "source": [
    "output, score = viterbi((pi, A, B), [Janet, will, back, the, Janet, back, bill])\n",
    "print([tag_dict[o] for o in output])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we've done the above algorithm correctly by implementing exhaustive search, which forms the cross-product of states^M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def exhaustive(params, words):\n",
    "    pi, A, B = params\n",
    "    N = len(words)\n",
    "    T = pi.shape[0]\n",
    "    \n",
    "    # track the running best sequence and its score\n",
    "    best = (None, float('-inf'))\n",
    "    # loop over the cartesian product of |states|^M\n",
    "    for ss in product(range(T), repeat=N):\n",
    "        # score the state sequence\n",
    "        score = pi[ss[0]] * B[ss[0], words[0]]\n",
    "        for i in range(1, N):\n",
    "            score *= A[ss[i-1], ss[i]] * B[ss[i], words[i]]\n",
    "        # update the running best\n",
    "        if score > best[1]:\n",
    "            best = (ss, score)\n",
    "            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'MD', 'VB', 'JJ', 'NN', 'RB', 'DT']\n",
      "2.0135707102213855e-15\n"
     ]
    }
   ],
   "source": [
    "output, score = exhaustive((pi, A, B), [Janet, will, back, the, bill])\n",
    "print([tag_dict[o] for o in tag_dict])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'MD', 'VB', 'JJ', 'NN', 'RB', 'DT']\n",
      "2.4671007551487507e-26\n"
     ]
    }
   ],
   "source": [
    "output, score = exhaustive((pi, A, B), [Janet, will, back, the, Janet, back, bill])\n",
    "print([tag_dict[o] for o in tag_dict])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it got the same results as before. Note that the exhaustive method is practical on anything beyond toy data due to the nasty cartesian product. But it is worth doing to verify the Viterbi code above is getting the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training, aka \"visible\" Markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the HMM parameters on the Penn Treebank, using the sample from NLTK. Note that this is a small fraction of the treebank, so we shouldn't expect great performance of our method trained only on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "corpus = treebank.tagged_sents()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first map words and tags to numbers for compatibility with the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "\n",
    "num_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "    \n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's hold out the last few sentences for testing, so that they are unseen during training and give a more reasonable estimate of accuracy on fresh text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = num_corpus[:-10] # reserve the last 10 sentences for testing\n",
    "testing = num_corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute relative frequency estimates based on the observed tag and word counts in the training set. Note that smoothing is important, here we add a small constant to all counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = len(tag_numbers)\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise\n",
    "eps = 0.1\n",
    "pi = eps * np.ones(S)\n",
    "A = eps * np.ones((S, S))\n",
    "B = eps * np.ones((S, V))\n",
    "\n",
    "# count\n",
    "for sent in training:\n",
    "    last_tag = None\n",
    "    for word, tag in sent:\n",
    "        B[tag, word] += 1\n",
    "        # bug fixed here 27/3/17; test was incorrect \n",
    "        if last_tag == None:\n",
    "            pi[tag] += 1\n",
    "        else:\n",
    "            A[last_tag, tag] += 1\n",
    "        last_tag = tag\n",
    "        \n",
    "# normalise\n",
    "pi /= np.sum(pi)\n",
    "for s in range(S):\n",
    "    B[s,:] /= np.sum(B[s,:])\n",
    "    A[s,:] /= np.sum(A[s,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to use our Viterbi method defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, score = viterbi((pi, A, B), list(map(lambda w_t: w_t[0], testing[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TOKEN\t TRUE\t PRED\n",
      "                   a\t   DT\t   DT\n",
      "               white\t  NNP\t  NNP\n",
      "               house\t  NNP\t  NNP\n",
      "           spokesman\t   NN\t   NN\n",
      "                said\t  VBD\t  VBD\n",
      "                last\t   JJ\t   JJ\n",
      "                week\t   NN\t   NN\n",
      "                that\t   IN\t   IN\n",
      "                 the\t   DT\t   DT\n",
      "           president\t   NN\t   NN\n",
      "                  is\t  VBZ\t  VBZ\n",
      "         considering\t  VBG\t  VBG\n",
      "                 *-1\t-NONE-\t-NONE-\n",
      "           declaring\t  VBG\t  VBG\n",
      "                that\t   IN\t   IN\n",
      "                 the\t   DT\t   DT\n",
      "        constitution\t  NNP\t  NNP\n",
      "          implicitly\t   RB\t  NNP\n",
      "               gives\t  VBZ\t  VBZ\n",
      "                 him\t  PRP\t  PRP\n",
      "                 the\t   DT\t   DT\n",
      "           authority\t   NN\t   NN\n",
      "                 for\t   IN\t   IN\n",
      "                   a\t   DT\t   DT\n",
      "           line-item\t   JJ\t   JJ\n",
      "                veto\t   NN\t   NN\n",
      "                 *-2\t-NONE-\t-NONE-\n",
      "                  to\t   TO\t   TO\n",
      "             provoke\t   VB\t   VB\n",
      "                   a\t   DT\t   DT\n",
      "                test\t   NN\t   NN\n",
      "                case\t   NN\t   NN\n",
      "                   .\t    .\t    .\n"
     ]
    }
   ],
   "source": [
    "print('%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED'))\n",
    "for (wi, ti), pi in zip(testing[0], predicted):\n",
    "    print('%20s\\t%5s\\t%5s' % (word_names[wi], tag_names[ti], tag_names[pi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad, only one error. Can you explain why this one might have occurred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
