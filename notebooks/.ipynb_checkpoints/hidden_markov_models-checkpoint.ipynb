{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook for COMP90042, Web search and Text Analysis*\n",
    "\n",
    "*Copyright The University of Melbourne, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll show how the Viterbi algorithm works for HMMs, assuming we have a trained model to start with. Further down we look at the forward and backward algorithms and Baum-Welch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model parameters based on the example from the lecture slides (values taken from figure). We have three observation types (up, down, unchanged) and also three hidden states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]])\n",
    "pi = np.array([0.5, 0.2, 0.3])\n",
    "O = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the example observation sequence UP, UP, DOWN, for which we'll try to discover the hidden state sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = UP, DOWN, UNCHANGED = 0, 1, 2\n",
    "observations = [UP, UP, DOWN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll code the Viterbi algorithm. It keeps a store of two components, the best scores to reach a state at a give time, and the last step of the path to get there. Scores alpha are initialised to -inf to denote that we haven't set them yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = np.zeros((len(observations), len(states))) # time steps x states\n",
    "alpha[:,:] = float('-inf')\n",
    "backpointers = np.zeros((len(observations), len(states)), 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base case for the recursion sets the starting state probs based on pi and generating the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base case, time step 0\n",
    "alpha[0, :] = pi * O[:,UP]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the recursive step, where we maximise over incoming transitions reusing the best incoming score, computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time step 1\n",
    "for t1 in states:\n",
    "    for t0 in states:\n",
    "        score = alpha[0, t0] * A[t0, t1] * O[t1,UP]\n",
    "        if score > alpha[1, t1]:\n",
    "            alpha[1, t1] = score\n",
    "            backpointers[1, t1] = t0\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the running maximum for any incoming state (t0) is maintained in alpha[1,t1], and the winning state is stored in addition, as a backpointer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with the next observation. (We'd do this as a loop over positions in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time step 2\n",
    "for t2 in states:\n",
    "    for t1 in states:\n",
    "        score = alpha[1, t1] * A[t1, t2] * O[t2,DOWN]\n",
    "        if score > alpha[2, t2]:\n",
    "            alpha[2, t2] = score\n",
    "            backpointers[2, t2] = t1\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read of the best final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t2 = np.argmax(alpha[2,:])\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out the rest of the path which is the best way to reach the final state, t2. We can work this out by taking a step backwards looking at the best incoming edge, i.e., as stored in the backpointers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = backpointers[2,t2]\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this until we reach the start of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = backpointers[1,t1]\n",
    "t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. The best state sequence is t=[0, 0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalising things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this all into a function to handle arbitrary length inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method on the same input, and a longer input observation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viterbi((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viterbi((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we've done the above algorithm correctly by implementing exhaustive search, which forms the cross-product of states^M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def exhaustive(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    # track the running best sequence and its score\n",
    "    best = (None, float('-inf'))\n",
    "    # loop over the cartesian product of |states|^M\n",
    "    for ss in product(range(S), repeat=M):\n",
    "        # score the state sequence\n",
    "        score = pi[ss[0]] * O[ss[0],observations[0]]\n",
    "        for i in range(1, M):\n",
    "            score *= A[ss[i-1], ss[i]] * O[ss[i], observations[i]]\n",
    "        # update the running best\n",
    "        if score > best[1]:\n",
    "            best = (ss, score)\n",
    "            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it got the same results as before. Note that the exhaustive method is practical on anything beyond toy data due to the nasty cartesian product. But it is worth doing to verify the Viterbi code above is getting the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training, aka \"visible\" Markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the HMM parameters on the Penn Treebank, using the sample from NLTK. Note that this is a small fraction of the treebank, so we shouldn't expect great performance of our method trained only on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = treebank.tagged_sents()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first map words and tags to numbers for compatibility with the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "\n",
    "num_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "    \n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's hold out the last few sentences for testing, so that they are unseen during training and give a more reasonable estimate of accuracy on fresh text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = num_corpus[:-10] # reserve the last 10 sentences for testing\n",
    "testing = num_corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute relative frequency estimates based on the observed tag and word counts in the training set. Note that smoothing is important, here we add a small constant to all counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = len(tag_numbers)\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise\n",
    "eps = 0.1\n",
    "pi = eps * np.ones(S)\n",
    "A = eps * np.ones((S, S))\n",
    "O = eps * np.ones((S, V))\n",
    "\n",
    "# count\n",
    "for sent in training:\n",
    "    last_tag = None\n",
    "    for word, tag in sent:\n",
    "        O[tag, word] += 1\n",
    "        if last_tag != None:\n",
    "            pi[tag] += 1\n",
    "        else:\n",
    "            A[last_tag, tag] += 1\n",
    "        last_tag = tag\n",
    "        \n",
    "# normalise\n",
    "pi /= np.sum(pi)\n",
    "for s in range(S):\n",
    "    O[s,:] /= np.sum(O[s,:])\n",
    "    A[s,:] /= np.sum(A[s,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to use our Viterbi method defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted, score = viterbi((pi, A, O), list(map(lambda w_t: w_t[0], testing[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED'))\n",
    "for (wi, ti), pi in zip(testing[0], predicted):\n",
    "    print('%20s\\t%5s\\t%5s' % (word_names[wi], tag_names[ti], tag_names[pi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad, only three errors. Can you explain why these might have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced material follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the notebook goes well beyond the material taught in the WSTA subject, but may interest the intrepid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalisation in Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related problem is marginalisation, when we wish to find the probability of an observation sequence *under any hidden state sequence*. This allows hidden Markov models to function as language models, but also is key to unsupervised training and the central algorithm for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Viterbi algorithm, we'll need to start with the mathematical definition and attempt to factorise it (to follow a recursion, thus allowing for dynamic programming). The quantity we wish to compute is $$p(\\vec{w}) = \\sum_{\\vec{t}} p(\\vec{t}, \\vec{w})$$\n",
    "where $w$ are the observations (words) and $t$ are the states (tags)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by expanding the summation \n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(\\vec{t}, \\vec{w})\n",
    "$$\n",
    "and expand the HMM probability \n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the full marginal probability $p(\\vec{w})$ and the probability up to position $N-1$, finishing with tag $t_{N-1}$\n",
    "$$p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look rather similar, and in fact we can express $p(\\vec{w})$ more simply as\n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_N} p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue further by defining $p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1})$ in terms of $p(w_1, w_2, \\ldots, w_{N-2}, t_{N-2})$ and so forth. (This is the same process used in the Viterbi algorithm, albeit swapping a max for a sum.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally we store a matrix of partial marginals, $\\alpha$ defined as follows\n",
    "$$\\alpha[i, t_i] = p(w_1, w_2, \\ldots, w_i, t_i)$$\n",
    "computed using the recursion\n",
    "$$  \n",
    "\\alpha[i, t_i] = \\sum_{t_{i-1}} \\alpha[i-1, t_i] p(t_i | t_{i-1}) p(w_i| t_i) \n",
    "$$\n",
    "and the base case for $i=1$,\n",
    "$$\n",
    "\\alpha[1, t_1] = p(t_1) p(w_1 | t_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have computed the formulation, we can put this into an iterative algorithm: compute the vector of alpha[1] values, then alpha[2] etc until we reach the end of our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for i in range(1, N):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                alpha[i, s2] += alpha[i-1, s1] * A[s1, s2] * O[s2, observations[i]]\n",
    "    \n",
    "    return (alpha, np.sum(alpha[N-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]])\n",
    "pi = np.array([0.5, 0.2, 0.3])\n",
    "O = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]])\n",
    "\n",
    "forward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we did this correctly by implementing an exhaustive equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exhaustive_forward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    total = 0.0\n",
    "    # loop over the cartesian product of |states|^N\n",
    "    for ss in product(range(S), repeat=N):\n",
    "        # score the state sequence\n",
    "        score = pi[ss[0]] * O[ss[0],observations[0]]\n",
    "        for i in range(1, N):\n",
    "            score *= A[ss[i-1], ss[i]] * O[ss[i], observations[i]]\n",
    "        total += score\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive_forward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive_forward((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process but working from left to right rather than right to left give us the backward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    beta = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    beta[N-1, :] = 1\n",
    "    \n",
    "    # recursive case\n",
    "    for i in range(N-2, -1, -1):\n",
    "        for s1 in range(S):\n",
    "            for s2 in range(S):\n",
    "                beta[i, s1] += beta[i+1, s2] * A[s1, s2] * O[s2, observations[i+1]]\n",
    "    \n",
    "    return (beta, np.sum(pi * O[:, observations[0]] * beta[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the it gets the same marginal probability as the forward algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised training of a HMM involves running forward and backward to estimate the *expected* probability of taking various state sequences, then updates the model to match these *expectations*. This repeats many times until things stabilise (covergence). Note that it's non-convex, so the starting point often affects the converged solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baum_welch(training, pi, A, O, iterations):\n",
    "    pi, A, O = np.copy(pi), np.copy(A), np.copy(O) # take copies, as we modify them\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    # do several steps of EM hill climbing\n",
    "    for it in range(iterations):\n",
    "        pi1 = np.zeros_like(pi)\n",
    "        A1 = np.zeros_like(A)\n",
    "        O1 = np.zeros_like(O)\n",
    "        \n",
    "        for observations in training:\n",
    "            # compute forward-backward matrices\n",
    "            alpha, za = forward((pi, A, O), observations)\n",
    "            beta, zb = backward((pi, A, O), observations)\n",
    "            assert abs(za - zb) < 1e-6, \"it's badness 10000 if the marginals don't agree\"\n",
    "            \n",
    "            # M-step here, calculating the frequency of starting state, transitions and (state, obs) pairs\n",
    "            pi1 += alpha[0,:] * beta[0,:] / za\n",
    "            for i in range(0, len(observations)):\n",
    "                O1[:, observations[i]] += alpha[i,:] * beta[i,:] / za\n",
    "            for i in range(1, len(observations)):\n",
    "                for s1 in range(S):\n",
    "                    for s2 in range(S):\n",
    "                        A1[s1, s2] += alpha[i-1,s1] * A[s1, s2] * O[s2, observations[i]] * beta[i,s2] / za\n",
    "                                                                    \n",
    "        # normalise pi1, A1, O1\n",
    "        pi = pi1 / np.sum(pi1)\n",
    "        for s in range(S):\n",
    "            A[s, :] = A1[s, :] / np.sum(A1[s, :])\n",
    "            O[s, :] = O1[s, :] / np.sum(O1[s, :])\n",
    "    \n",
    "    return pi, A, O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out by training on our example from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi2, A2, O2 = baum_welch([[UP, UP, DOWN]], pi, A, O, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi2, A2, O2), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it memorised the sequence, and has assigned it a very high probability. The downside is that it won't be very accepting of other sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi2, A2, O2), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks strangely reminiscent of many other learning problems.... Can you think how we might deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally training on both sequences leads to perhaps a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi3, A3, O3 = baum_welch([[UP, UP, DOWN], [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP]], pi, A, O, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi3, A3, O3), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi3, A3, O3), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping (making?) in real"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
