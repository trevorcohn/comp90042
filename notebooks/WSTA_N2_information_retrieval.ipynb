{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook demonstrates how to put together a simple retrieval engine in python. We'll focus on *ranked retrieval*, using a TFxIDF vector space model. We won't bother about any optimisations, and just use python dicts, sets etc.\n",
    "\n",
    "Our dataset is the *Cranfield collection*, an early resource which has been widely used in IR research. You may need to call `nltk.download()` to install the stopwords corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import requests, tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, download the cranfield files, using the code below. Or you can do this manually by downloading http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz and extracting the files into the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# download\n",
    "fname = 'cran.tar.gz'\n",
    "url = 'http://ir.dcs.gla.ac.uk/resources/test_collections/cran/' + fname\n",
    "r = requests.get(url)\n",
    "# write to disk\n",
    "open(fname , 'wb').write(r.content)\n",
    "# extract files from archive\n",
    "tar = tarfile.open(fname, \"r:\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll need to extract the document details---the document ids and text content---from the cranfield files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_cranfield_documents(fname):\n",
    "    SKIP, READ = range(2)\n",
    "    state = SKIP\n",
    "    identifier = body = None\n",
    "    with open(fname) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            parts = line.split()\n",
    "            first = parts[0]\n",
    "            if first[0] == \".\":\n",
    "                if first[1] == \"I\":\n",
    "                    if state != None and identifier:\n",
    "                        yield (identifier, body)\n",
    "                    identifier = int(parts[1])\n",
    "                    state = SKIP\n",
    "                elif first[1] == \"W\":\n",
    "                    state = READ\n",
    "                    body = ''\n",
    "                else:\n",
    "                    state = SKIP\n",
    "            elif state == READ:\n",
    "                body += line + ' '\n",
    "        if state == READ:\n",
    "            yield (identifier, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'experimental investigation of the aerodynamics of a wing in a slipstream . an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem . the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary-layer-control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory . an empirical evaluation of the destalling effects was made for the specific configuration of the experiment . ')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docfname = 'cran.all.1400'\n",
    "raw_docs = list(parse_cranfield_documents(docfname))\n",
    "len(raw_docs)\n",
    "raw_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 'the buckling shear stress of simply-supported infinitely long plates with transverse stiffeners . this report is an extension of previous theoretical investigations of the elastic buckling in shear of flat plates reinforced by transverse stiffeners . the plates are treated as infinitely long and simply-supported along the long sides . stiffeners are spaced at regular intervals, dividing the plate into a number of panels of uniform size . the effect ob bending and torsional stiffnesses of the stiffener upon the buckling shear stress is calculated for the complete range of stiffnesses, for panels with ratios of width to stiffener spacing of graphical forms . ')\n"
     ]
    }
   ],
   "source": [
    "print(raw_docs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll need to tokenise the documents, remove stop-words and stem the words to form our bag-of-words representation. Here we'll use the PorterStemmer (there are others in NLTK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_terms(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's test the method using the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', '/destalling/', 'aerodynam', 'agre', 'angl', 'attack', 'basi', 'boundary-layer-control', 'compar', 'configur', 'curv', 'destal', 'determin', 'differ', 'distribut', 'due', 'effect', 'empir', 'evalu', 'evid', 'experi', 'experiment', 'flow', 'found', 'free', 'increas', 'increment', 'integr', 'intend', 'investig', 'lift', 'load', 'made', 'order', 'part', 'potenti', 'problem', 'produc', 'propel', 'ratio', 'remain', 'result', 'show', 'slipstream', 'span', 'spanwis', 'specif', 'stream', 'studi', 'substanti', 'subtract', 'support', 'theoret', 'theori', 'togeth', 'treatment', 'veloc', 'well', 'wing']\n"
     ]
    }
   ],
   "source": [
    "print(list(sorted(extract_terms(raw_docs[0][1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We probably want to remove numbers and punctuation, which aren't being caught by the stop list. We may want to be a bit more agressive with tokenising hyphenated words (although take care, as some might be important.) Have a go yourself and see if you can improve the preprocessing to correct for these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can apply the term extraction method to all documents in our corpus. (This may take a minute.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for docid, text in raw_docs:\n",
    "    terms = extract_terms(text)\n",
    "    docs[docid] = terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To build a _vector space model_ we must define a scoring function capable of relating a query to a document, then use this to retrieve the top *k* scoring documents for a given query. The scoring function we use is the cosine similarity measure over a _TF*IDF_ vector representation of the document and the query. \n",
    "\n",
    "This requires us to process the data to compute:\n",
    "1. term frequencies within each document (a *bag* of words, rather than a *set* as for boolean retrieval)\n",
    "2. document frequencies for each term, counting how many documents they occur in\n",
    "3. normlised _tf*idf_ vectors for each document \n",
    "4. an inverted index to allow for efficient lookup by term\n",
    "\n",
    "The first step is to collect term frequencies for each document, which is only a slight change from the code above for creating the binary index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll also need to compute the document frequencies, for the *idf* factors in the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the above two functions, process the corpus into term frequencies and document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_term_freqs = {}\n",
    "for docid, terms in docs.items():\n",
    "    term_freqs = extract_term_freqs(terms)\n",
    "    doc_term_freqs[docid] = term_freqs\n",
    "M = len(doc_term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_freqs = compute_doc_freqs(doc_term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As a sanity check, let's check some of the *df* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term flux df 16 idf 4.471638793363569\n",
      "term viscous df 117 idf 2.482053580805594\n",
      "term magnet df 37 idf 3.6333096029591254\n"
     ]
    }
   ],
   "source": [
    "for term in \"flux viscous magnet\".split():\n",
    "    stem = stemmer.stem(term.lower())\n",
    "    print(\"term\", term, \"df\", doc_freqs[stem], 'idf', log(M / float(doc_freqs[stem])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now to create the inverted index. For this we need to include the scoring function to compute the *tf* and *idf* values, then normalise each document vector to be unit length. For this we process each document in the corpus, compute a vector with one entry per term, normalise for length and store in the inverted index.\n",
    "\n",
    "Note that this a fairly inefficient inverted index. Many optimisations are possible, and are indeed necessary for this to scale to more realistic sized datasets. The approach below, though, is fine for small collections like this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vsm_inverted_index = defaultdict(list)\n",
    "for docid, term_freqs in doc_term_freqs.items():\n",
    "    N = sum(term_freqs.values())\n",
    "    length = 0\n",
    "    \n",
    "    # find tf*idf values and accumulate sum of squares \n",
    "    tfidf_values = []\n",
    "    for term, count in term_freqs.items():\n",
    "        tfidf = float(count) * log(M / float(doc_freqs[term]))\n",
    "        tfidf_values.append((term, tfidf))\n",
    "        length += tfidf ** 2\n",
    "\n",
    "    # normalise documents by length and insert into index\n",
    "    length = length ** 0.5\n",
    "    for term, tfidf in tfidf_values:\n",
    "        # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "        vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "        \n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for term, docids in vsm_inverted_index.items():\n",
    "    docids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For querying this index, we use the algorithm from the lecture. This just sums up the weights for each document using the posting lists for all query terms, then returns the ranked list of results. (Again, there are more efficient algorithms for doing ranked retrieval in the VSM, such as [WAND](http://lucene.472066.n3.nabble.com/attachment/577044/0/p426-broder.pdf).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(298, 0.0009694331486352642),\n",
       " (208, 0.0009694244092723588),\n",
       " (1253, 0.0009693369541745064),\n",
       " (653, 0.0009693353264029864),\n",
       " (967, 0.000969333754779725),\n",
       " (112, 0.0009693326008957937),\n",
       " (1273, 0.0009693318118730736),\n",
       " (299, 0.0009693314295824477),\n",
       " (267, 0.0009693277699471529),\n",
       " (1250, 0.0008955698481602709)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_vsm(query, index, k=10):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "results = query_vsm([stemmer.stem(term.lower()) for term in \"flux viscous magnet\".split()], vsm_inverted_index)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's worth taking a look at the top scoring document(s), e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 'incompressible wedge flows of an electrically conducting viscous fluid in the presence of a magnetic field . the purpose of this note is to discuss the two-dimensional flow of an electrically conducting viscous fluid past a wedge in the presence of a magnetic field .  the governing differential equations and boundary conditions are given and analyzed . ')\n"
     ]
    }
   ],
   "source": [
    "print(raw_docs[results[0][0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that this document does not include all the query terms. While it has *viscous* and *magnet* it does not include *flux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This collection contains other files, besides the lists of documents. Namely a list of sample queries, and judgements of the relevance of each document against these queries. We'll return to this in later weeks when covering IR evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
