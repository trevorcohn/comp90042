{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook for COMP90042, Web search and Text Analysis*\n",
    "\n",
    "*Copyright The University of Melbourne, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will check the performance of the POS tagger from the last workshop on a different domain: Twitter. First, let's build the HMM tagger again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import treebank\n",
    "corpus = treebank.tagged_sents()\n",
    "\n",
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "\n",
    "num_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "    \n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag\n",
    "    \n",
    "S = len(tag_numbers)\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise\n",
    "eps = 0.1\n",
    "pi = eps * np.ones(S)\n",
    "A = eps * np.ones((S, S))\n",
    "O = eps * np.ones((S, V))\n",
    "\n",
    "# count\n",
    "for sent in num_corpus:\n",
    "    last_tag = None\n",
    "    for word, tag in sent:\n",
    "        O[tag, word] += 1\n",
    "        if last_tag == None:\n",
    "            pi[tag] += 1\n",
    "        else:\n",
    "            A[last_tag, tag] += 1\n",
    "        last_tag = tag\n",
    "        \n",
    "# normalise\n",
    "pi /= np.sum(pi)\n",
    "for s in range(S):\n",
    "    O[s,:] /= np.sum(O[s,:])\n",
    "    A[s,:] /= np.sum(A[s,:])\n",
    "    \n",
    "    \n",
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a corpus of POS tagged tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the lecture: we always need some annotated data in order to evaluate our methods, even when they are unsupervised. In order to do this, we will use a dataset of tweets annotated with POS tags, which we will download automatically via Python.\n",
    "\n",
    "The next step is to read the file. We will use it as a *test set* only: you are not allowed to use any of this data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "urllib.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "with open('pos.txt') as f:\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            test_inputs.append(words)\n",
    "            test_outputs.append(pos_tags)\n",
    "            words = []\n",
    "            pos_tags = []\n",
    "        else:\n",
    "            word, pos = line.strip().split()\n",
    "            words.append(word)\n",
    "            pos_tags.append(pos)\n",
    "    \n",
    "print test_inputs[0]\n",
    "print test_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging the corpus and evaluating it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we read our test set, let's try to tag it using our HMM tagger trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sent in test_inputs:\n",
    "    encoded_sent = [word_numbers[w] for w in sent]\n",
    "    pred = viterbi((pi, A, O), encoded_sent)\n",
    "    predictions.append([tag_names[i] for i in pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will raise an error due to an OOV word. A simple way to deal with OOV's is to smooth the counts in the emission matrix. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an OOV token to our dictionary. Let's call it '<unk>'\n",
    "unk_index = len(word_numbers)\n",
    "word_numbers.setdefault('<unk>', unk_index)\n",
    "word_names.append('<unk>')\n",
    "\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise\n",
    "eps = 0.1\n",
    "O = eps * np.ones((S, V))\n",
    "\n",
    "# add one smoothing\n",
    "O += 1.0\n",
    "\n",
    "# count\n",
    "for sent in num_corpus:\n",
    "    for word, tag in sent:\n",
    "        O[tag, word] += 1\n",
    " \n",
    "# normalise\n",
    "for s in range(S):\n",
    "    O[s,:] /= np.sum(O[s,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to tag the sentence, we first replace any OOV words with our '<unk>' token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sent in test_inputs:\n",
    "    encoded_sent = []\n",
    "    for word in sent:\n",
    "        if word in word_numbers:\n",
    "            encoded_sent.append(word_numbers[word])\n",
    "        else:\n",
    "            encoded_sent.append(word_numbers['<unk>'])\n",
    "    pred, _ = viterbi((pi, A, O), encoded_sent)\n",
    "    #predictions.append([tag_names[i] for i in predicted]\n",
    "    predictions.append(pred)\n",
    "    \n",
    "\n",
    "print predictions[0]\n",
    "print('%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED'))\n",
    "for wi, ti, predi in zip(test_inputs[0], test_outputs[0], predictions[0]):\n",
    "    print('%20s\\t%5s\\t%5s' % (wi, ti, tag_names[predi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few errors here, much more than in the previous workshop example. Let's try to quantify this in terms of accuracy, so we can compare with PTB numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "# flat our data into single lists\n",
    "all_test_tags = [tag for tags in test_outputs for tag in tags]\n",
    "# for predictions, we need to obtain the original tag from the index\n",
    "all_pred_tags = [tag_names[tag] for tags in predictions for tag in tags]\n",
    "\n",
    "print acc(all_test_tags, all_pred_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51.9% accuracy is quite low. Compare this to the performance on Penn Treebank, which can reach 96.7% accuracy. One reason for such low numbers is the fact we are training only on a subset of Penn Treebank (since it is freely available on NLTK). But even state-of-the-art POS taggers reach only 80% accuracy on this dataset (Ritter et al., EMNLP 2011).\n",
    "\n",
    "Notice that the twitter test set has some tags which are not defined in PTB, such as \"USR\" for user mentions (@paulwalk, in the example above). This means that these additional tags will never get predicted using the current tagger. Can you come up with a solution for that?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
